{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1cwC8/vvHkZ99Q9dTkgnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearning-Y4/TrafficSignRecognition/blob/TrafficSignRecognition_CNN_model/TrafficSignRecognition_CNN_model_GTSRB_Dataset_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrafficSignRecognition_CNN_model - GTSRB Dataset Version"
      ],
      "metadata": {
        "id": "-MqKxvwoCr-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "9rwspLOCC8OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y68PzBPSC6eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip GTSRB data set"
      ],
      "metadata": {
        "id": "A5uQy6iNDC0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip GTSRB data set\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/gtsrb/gtsrb.zip'\n",
        "extract_path = '/content/gtsrb'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset extracted successfully!\")\n",
        "else:\n",
        "    print(\"Dataset already extracted!\")"
      ],
      "metadata": {
        "id": "4dVTNsp4DDZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ===== Load and Explore Dataset ====="
      ],
      "metadata": {
        "id": "4pjMOgGVDcj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Load and Explore Dataset =====\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_gtsrb_data(data_dir, csv_file=None):\n",
        "    \"\"\"\n",
        "    Load GTSRB dataset from folder structure\n",
        "    Folder structure: Train/0/, Train/1/, ..., Train/42/\n",
        "    Or Test/0.png, Test/1.png with Test.csv\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # If CSV file is provided (for test set)\n",
        "    if csv_file and os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        for idx, row in df.iterrows():\n",
        "            img_path = os.path.join(data_dir, row['Path'])\n",
        "            if os.path.exists(img_path):\n",
        "                img = Image.open(img_path)\n",
        "                img = img.resize((32, 32))  # Resize to 32x32\n",
        "                images.append(np.array(img))\n",
        "                labels.append(row['ClassId'])\n",
        "    else:\n",
        "        # Load from folder structure (for training set)\n",
        "        for class_id in range(43):  # GTSRB has 43 classes (0-42)\n",
        "            class_folder = os.path.join(data_dir, str(class_id))\n",
        "            if not os.path.exists(class_folder):\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(class_folder):\n",
        "                if img_name.endswith(('.png', '.ppm', '.jpg')):\n",
        "                    img_path = os.path.join(class_folder, img_name)\n",
        "                    try:\n",
        "                        img = Image.open(img_path)\n",
        "                        img = img.resize((32, 32))  # Resize to 32x32\n",
        "                        images.append(np.array(img))\n",
        "                        labels.append(class_id)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "daVM3O0TDbha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "print(\"Loading training data...\")\n",
        "X_train_full, y_train_full = load_gtsrb_data('/content/gtsrb/Train')\n",
        "\n",
        "# Split into train and validation (80-20 split)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_train_full\n",
        ")\n",
        "\n",
        "# Load test data\n",
        "print(\"Loading test data...\")\n",
        "# Try loading from Test.csv if available\n",
        "test_csv_path = '/content/gtsrb/Test.csv'\n",
        "if os.path.exists(test_csv_path):\n",
        "    X_test, y_test = load_gtsrb_data('/content/gtsrb/Test', test_csv_path)\n",
        "else:\n",
        "    # If no test set, use part of training data\n",
        "    X_test, y_test = load_gtsrb_data('/content/gtsrb/Test')\n",
        "    if len(X_test) == 0:\n",
        "        print(\"No test set found. Using 10% of training data as test set.\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "print('Training set:', X_train.shape)\n",
        "print('Validation set:', X_valid.shape)\n",
        "print('Test set:', X_test.shape)\n",
        "print('Number of classes:', len(np.unique(y_train)))"
      ],
      "metadata": {
        "id": "sYtCHjl0DuFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview some images\n",
        "fig, axs = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i in range(10):\n",
        "    axs[i//5, i%5].imshow(X_train[i])\n",
        "    axs[i//5, i%5].set_title(f\"Label: {y_train[i]}\")\n",
        "    axs[i//5, i%5].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJ3m3AXAD5FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Data Preprocessing =====\n",
        "import cv2\n",
        "\n",
        "# Convert to grayscale\n",
        "print(\"Converting to grayscale...\")\n",
        "X_train_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_train])\n",
        "X_valid_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_valid])\n",
        "X_test_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_test])\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train_gray = X_train_gray / 255.0\n",
        "X_valid_gray = X_valid_gray / 255.0\n",
        "X_test_gray = X_test_gray / 255.0\n",
        "\n",
        "# Reshape for CNN input (N, H, W, 1)\n",
        "X_train_gray = X_train_gray.reshape(X_train_gray.shape + (1,))\n",
        "X_valid_gray = X_valid_gray.reshape(X_valid_gray.shape + (1,))\n",
        "X_test_gray = X_test_gray.reshape(X_test_gray.shape + (1,))\n",
        "\n",
        "print(\"Preprocessing complete!\")\n",
        "print(\"Training shape:\", X_train_gray.shape)"
      ],
      "metadata": {
        "id": "Ldpw6WM5D9nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Build Optimized CNN Model =====\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    # First Convolutional Block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001),\n",
        "                  input_shape=(32, 32, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(43, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "dMH37JxdEB3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Compile Model =====\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ===== Set up Callbacks =====\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=8,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate when validation loss plateaus\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=4,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ===== Train Model =====\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(\n",
        "    X_train_gray, y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(X_valid_gray, y_valid),\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "RPos7g9fFdt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Evaluate Performance =====\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_acc = model.evaluate(X_test_gray, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "# Evaluate on training and validation sets for comparison\n",
        "train_loss, train_acc = model.evaluate(X_train_gray, y_train, verbose=0)\n",
        "val_loss, val_acc = model.evaluate(X_valid_gray, y_valid, verbose=0)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "UBbwZ64YEOPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Plot Training History =====\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Loss', fontsize=12)\n",
        "ax2.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJgNV6KWERHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Confusion Matrix & Classification Report =====\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\nGenerating predictions...\")\n",
        "y_pred = np.argmax(model.predict(X_test_gray, verbose=0), axis=1)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm, cmap='Blues', annot=False, fmt='d', cbar_kws={'label': 'Count'})\n",
        "plt.title(\"Confusion Matrix - CNN Model\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# ===== Performance Metrics Summary =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_dict = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision (Macro)': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'Recall (Macro)': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'F1-Score (Macro)': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'Precision (Weighted)': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "    'Recall (Weighted)': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "    'F1-Score (Weighted)': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "}\n",
        "\n",
        "for metric, value in metrics_dict.items():\n",
        "    print(f\"{metric:.<40} {value:.4f} ({value*100:.2f}%)\")\n"
      ],
      "metadata": {
        "id": "ZScDB4NIEW5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Save the Model =====\n",
        "model.save('/content/drive/MyDrive/gtsrb/model/traffic_sign_cnn_optimized.h5')\n",
        "print(\"\\nModel saved successfully to Google Drive!\")"
      ],
      "metadata": {
        "id": "Rv-gXzRIEgrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a text file\n",
        "with open('/content/drive/MyDrive/gtsrb/model/cnn_results.txt', 'w') as f:\n",
        "    f.write(\"CNN Model Performance Results\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\\n\")\n",
        "    f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
        "    for metric, value in metrics_dict.items():\n",
        "        f.write(f\"{metric}: {value:.4f} ({value*100:.2f}%)\\n\")\n",
        "\n",
        "print(\"Results saved to cnn_results.txt in Google Drive!\")"
      ],
      "metadata": {
        "id": "Xdi6iUlnEjm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}