{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vxeux_cnDCNQqf6akcjWeY2wZQf95Fm-",
      "authorship_tag": "ABX9TyPtHGUCxvKcxvKGXhAFy+kb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearning-Y4/TrafficSignRecognition/blob/main/TrafficSignRecognition_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrafficSignRecognition_CNN_model"
      ],
      "metadata": {
        "id": "kl_FR5Y02DSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Explore Dataset\n",
        "\n",
        "The dataset already has preprocessed training and testing .p (pickle) files."
      ],
      "metadata": {
        "id": "okw8uZLmVI5r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krxHP4_O1R3j"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the dataset"
      ],
      "metadata": {
        "id": "87wSIVQRQkdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/archive.zip'\n",
        "extract_path = '/content/traffic_signs'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"Dataset extracted successfully!\")\n",
        "else:\n",
        "    print(\"Dataset already extracted!\")"
      ],
      "metadata": {
        "id": "zQ5eTwBbQiFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Explore Dataset"
      ],
      "metadata": {
        "id": "z_Qa116QQzYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Load and Explore Dataset =====\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load training and testing data\n",
        "with open('/content/traffic_signs/train.p', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open('/content/traffic_signs/valid.p', 'rb') as f:\n",
        "    valid_data = pickle.load(f)\n",
        "with open('/content/traffic_signs/test.p', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# Extract images and labels\n",
        "X_train, y_train = train_data['features'], train_data['labels']\n",
        "X_valid, y_valid = valid_data['features'], valid_data['labels']\n",
        "X_test, y_test = test_data['features'], test_data['labels']\n",
        "\n",
        "print('Training set:', X_train.shape)\n",
        "print('Validation set:', X_valid.shape)\n",
        "print('Test set:', X_test.shape)\n",
        "print('Number of classes:', len(np.unique(y_train)))"
      ],
      "metadata": {
        "id": "ybXHrzG5FrVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview some images:"
      ],
      "metadata": {
        "id": "7_MNaELVF1fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview some images\n",
        "fig, axs = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i in range(10):\n",
        "    axs[i//5, i%5].imshow(X_train[i])\n",
        "    axs[i//5, i%5].set_title(f\"Label: {y_train[i]}\")\n",
        "    axs[i//5, i%5].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tYJuROs9F5sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "Normalize and convert to grayscale (optional but improves performance):"
      ],
      "metadata": {
        "id": "zsl6NODoF9cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Data Preprocessing =====\n",
        "import cv2\n",
        "\n",
        "# Convert to grayscale\n",
        "print(\"Converting to grayscale...\")\n",
        "X_train_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_train])\n",
        "X_valid_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_valid])\n",
        "X_test_gray  = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_test])\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train_gray = X_train_gray / 255.0\n",
        "X_valid_gray = X_valid_gray / 255.0\n",
        "X_test_gray  = X_test_gray / 255.0\n",
        "\n",
        "# Reshape for CNN input (N, H, W, 1)\n",
        "X_train_gray = X_train_gray.reshape(X_train_gray.shape + (1,))\n",
        "X_valid_gray = X_valid_gray.reshape(X_valid_gray.shape + (1,))\n",
        "X_test_gray  = X_test_gray.reshape(X_test_gray.shape + (1,))\n",
        "\n",
        "print(\"Preprocessing complete!\")\n",
        "print(\"Training shape:\", X_train_gray.shape)\n"
      ],
      "metadata": {
        "id": "rcYicA2bGPcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the CNN Model\n",
        "We'll build a simple CNN similar to LeNet architecture:"
      ],
      "metadata": {
        "id": "2ksQ4uDcGSwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Build Optimized CNN Model =====\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    # First Convolutional Block\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001),\n",
        "                  input_shape=(32,32,1)),\n",
        "    layers.BatchNormalization(),\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(43, activation='softmax')  # 43 traffic sign classes\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "iOb9QDezGhrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "GJyaXTO3GlTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Compile Model =====\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ===== Set up Callbacks =====\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=8,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate when validation loss plateaus\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=4,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ===== Train Model =====\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(\n",
        "    X_train_gray, y_train,\n",
        "    epochs=50,\n",
        "    validation_data=(X_valid_gray, y_valid),\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "OlMgTgxuGpPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Performance"
      ],
      "metadata": {
        "id": "25VZbpQAGsIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Evaluate Performance =====\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_acc = model.evaluate(X_test_gray, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "# Evaluate on training and validation sets for comparison\n",
        "train_loss, train_acc = model.evaluate(X_train_gray, y_train, verbose=0)\n",
        "val_loss, val_acc = model.evaluate(X_valid_gray, y_valid, verbose=0)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "wFPGIOopGuVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot accuracy/loss curves:"
      ],
      "metadata": {
        "id": "dI86p6FSGyNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Plot Training History =====\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Loss', fontsize=12)\n",
        "ax2.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kDXLF9XwG0Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix & Classification Report"
      ],
      "metadata": {
        "id": "4xJIj4G9G2jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# ===== Performance Metrics Summary =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_dict = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision (Macro)': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'Recall (Macro)': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'F1-Score (Macro)': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "    'Precision (Weighted)': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "    'Recall (Weighted)': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "    'F1-Score (Weighted)': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "}\n",
        "\n",
        "for metric, value in metrics_dict.items():\n",
        "    print(f\"{metric:.<40} {value:.4f} ({value*100:.2f}%)\")\n"
      ],
      "metadata": {
        "id": "7Rz15IzOG5Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Model"
      ],
      "metadata": {
        "id": "wjTzjqIaG8ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/traffic_sign_cnn_optimized.h5')\n",
        "print(\"\\nModel saved successfully to Google Drive!\")"
      ],
      "metadata": {
        "id": "j3rjysX2G-Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save results to a text file"
      ],
      "metadata": {
        "id": "DqufJeizU1dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a text file\n",
        "with open('/content/drive/MyDrive/cnn_results.txt', 'w') as f:\n",
        "    f.write(\"CNN Model Performance Results\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\\n\")\n",
        "    f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
        "    for metric, value in metrics_dict.items():\n",
        "        f.write(f\"{metric}: {value:.4f} ({value*100:.2f}%)\\n\")\n",
        "\n",
        "print(\"Results saved to cnn_results.txt in Google Drive!\")"
      ],
      "metadata": {
        "id": "lSBT1MXzU0Fh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}